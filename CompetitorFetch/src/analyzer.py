"""
NLPåˆ†ææ ¸å¿ƒæ¨¡å—
åŸºäºSnowNLPå’Œjiebaå®ç°æƒ…æ„Ÿåˆ†æå’Œå…³é”®è¯æå–
"""
import jieba
import jieba.analyse
from snownlp import SnowNLP
from typing import List, Dict, Tuple
from collections import Counter
import logging
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FeedbackAnalyzer:
    """ç”¨æˆ·åé¦ˆåˆ†æå™¨"""
    
    def __init__(self, custom_dict_path: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            custom_dict_path: è‡ªå®šä¹‰è¯å…¸è·¯å¾„
        """
        # åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆäº§å“ç›¸å…³æœ¯è¯­ï¼‰
        if custom_dict_path and os.path.exists(custom_dict_path):
            jieba.load_userdict(custom_dict_path)
            logger.info(f"å·²åŠ è½½è‡ªå®šä¹‰è¯å…¸: {custom_dict_path}")
        
        # æ·»åŠ å¸¸è§äº§å“åé¦ˆè¯æ±‡
        self._add_product_keywords()
        
        logger.info("FeedbackAnalyzer åˆå§‹åŒ–å®Œæˆ")
    
    def _add_product_keywords(self):
        """æ·»åŠ äº§å“ç›¸å…³å…³é”®è¯åˆ°jiebaè¯å…¸"""
        product_keywords = [
            'å¡é¡¿', 'é—ªé€€', 'å´©æºƒ', 'åŠ è½½æ…¢', 'å“åº”æ…¢',
            'åŠŸèƒ½ç¼ºå¤±', 'æ“ä½œå¤æ‚', 'ç•Œé¢æ··ä¹±', 'ä¸å¥½ç”¨',
            'æ€§ä»·æ¯”', 'ç”¨æˆ·ä½“éªŒ', 'äº¤äº’è®¾è®¡', 'è§†è§‰è®¾è®¡',
            'æ˜“ç”¨æ€§', 'ç¨³å®šæ€§', 'å…¼å®¹æ€§', 'æµç•…åº¦'
        ]
        
        for word in product_keywords:
            jieba.add_word(word, freq=10000)
    
    def analyze_sentiment(self, text: str) -> Dict:
        """
        åˆ†æå•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            
        Returns:
            æƒ…æ„Ÿåˆ†æç»“æœå­—å…¸
        """
        try:
            s = SnowNLP(text)
            sentiment_score = s.sentiments  # 0-1ä¹‹é—´ï¼Œè¶Šæ¥è¿‘1è¶Šç§¯æ
            
            # åˆ†ç±»è§„åˆ™
            if sentiment_score >= 0.6:
                sentiment = "æ­£é¢"
                emotion = "ğŸ˜Š"
            elif sentiment_score >= 0.4:
                sentiment = "ä¸­æ€§"
                emotion = "ğŸ˜"
            else:
                sentiment = "è´Ÿé¢"
                emotion = "ğŸ˜"
            
            return {
                "text": text,
                "sentiment": sentiment,
                "sentiment_score": round(sentiment_score, 4),
                "emotion": emotion
            }
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}")
            return {
                "text": text,
                "sentiment": "æœªçŸ¥",
                "sentiment_score": 0.5,
                "emotion": "â“"
            }
    
    def batch_analyze_sentiment(self, texts: List[str]) -> List[Dict]:
        """
        æ‰¹é‡åˆ†ææƒ…æ„Ÿ
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ‰¹é‡æƒ…æ„Ÿåˆ†æï¼Œå…± {len(texts)} æ¡")
        results = []
        
        for i, text in enumerate(texts, 1):
            if i % 100 == 0:
                logger.info(f"å·²å¤„ç† {i}/{len(texts)}")
            results.append(self.analyze_sentiment(text))
        
        logger.info("æ‰¹é‡æƒ…æ„Ÿåˆ†æå®Œæˆ")
        return results
    
    def extract_keywords(self, text: str, topK: int = 10) -> List[Tuple[str, float]]:
        """
        ä»å•æ¡æ–‡æœ¬æå–å…³é”®è¯
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            topK: è¿”å›å‰Kä¸ªå…³é”®è¯
            
        Returns:
            å…³é”®è¯åŠæƒé‡åˆ—è¡¨ [(è¯, æƒé‡), ...]
        """
        try:
            # ä½¿ç”¨TF-IDFæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(
                text, 
                topK=topK, 
                withWeight=True,
                allowPOS=('n', 'v', 'vn', 'a', 'an')  # åªä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
            )
            return keywords
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {str(e)}")
            return []
    
    def extract_pain_points(self, texts: List[str], topK: int = 20) -> List[Tuple[str, int]]:
        """
        ä»æ‰€æœ‰åé¦ˆä¸­æå–é«˜é¢‘ç—›ç‚¹è¯æ±‡
        
        Args:
            texts: åé¦ˆæ–‡æœ¬åˆ—è¡¨
            topK: è¿”å›å‰Kä¸ªé«˜é¢‘è¯
            
        Returns:
            ç—›ç‚¹è¯åŠé¢‘æ¬¡ [(è¯, é¢‘æ¬¡), ...]
        """
        logger.info(f"å¼€å§‹æå–ç—›ç‚¹è¯æ±‡ï¼Œå…± {len(texts)} æ¡åé¦ˆ")
        
        # ç—›ç‚¹ç›¸å…³çš„è´Ÿé¢è¯æ±‡
        pain_point_indicators = [
            'å¡', 'æ…¢', 'é—ªé€€', 'å´©æºƒ', 'å¤±è´¥', 'é”™è¯¯', 'é—®é¢˜',
            'ç¼ºå°‘', 'ç¼ºå¤±', 'æ²¡æœ‰', 'ä¸èƒ½', 'æ— æ³•', 'éš¾', 'å¤æ‚',
            'å·®', 'åƒåœ¾', 'çƒ‚', 'bug', 'æ•…éšœ', 'å¡é¡¿', 'å»¶è¿Ÿ'
        ]
        
        all_words = []
        
        for text in texts:
            # åªåˆ†æè´Ÿé¢æˆ–ä¸­æ€§åé¦ˆ
            sentiment = self.analyze_sentiment(text)
            if sentiment['sentiment_score'] < 0.6:  # éæ­£é¢åé¦ˆ
                words = jieba.lcut(text)
                # è¿‡æ»¤é•¿åº¦å’Œåœç”¨è¯
                words = [w for w in words if len(w) >= 2 and w not in self._get_stopwords()]
                all_words.extend(words)
        
        # ç»Ÿè®¡è¯é¢‘
        word_counter = Counter(all_words)
        
        # ä¼˜å…ˆæå–åŒ…å«ç—›ç‚¹æŒ‡ç¤ºè¯çš„çŸ­è¯­
        pain_points = []
        for word, count in word_counter.most_common(topK * 2):
            # å¦‚æœè¯è¯­æœ¬èº«æ˜¯ç—›ç‚¹æŒ‡ç¤ºè¯ï¼Œæˆ–åŒ…å«ç—›ç‚¹ç‰¹å¾
            if any(indicator in word for indicator in pain_point_indicators):
                pain_points.append((word, count))
            elif count >= 2:  # å‡ºç°æ¬¡æ•°è¶³å¤Ÿå¤š
                pain_points.append((word, count))
        
        logger.info(f"æå–åˆ° {len(pain_points[:topK])} ä¸ªé«˜é¢‘ç—›ç‚¹")
        return pain_points[:topK]
    
    def _get_stopwords(self) -> set:
        """è·å–åœç”¨è¯åˆ—è¡¨"""
        stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£'
        }
        return stopwords
    
    def generate_summary(self, analysis_results: List[Dict]) -> Dict:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦
        
        Args:
            analysis_results: æƒ…æ„Ÿåˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡æ‘˜è¦
        """
        total = len(analysis_results)
        if total == 0:
            return {}
        
        sentiment_counter = Counter([r['sentiment'] for r in analysis_results])
        
        positive_count = sentiment_counter.get('æ­£é¢', 0)
        neutral_count = sentiment_counter.get('ä¸­æ€§', 0)
        negative_count = sentiment_counter.get('è´Ÿé¢', 0)
        
        summary = {
            "total_feedback": total,
            "positive_count": positive_count,
            "neutral_count": neutral_count,
            "negative_count": negative_count,
            "positive_ratio": round(positive_count / total * 100, 2),
            "neutral_ratio": round(neutral_count / total * 100, 2),
            "negative_ratio": round(negative_count / total * 100, 2),
            "avg_sentiment_score": round(
                sum(r['sentiment_score'] for r in analysis_results) / total, 4
            )
        }
        
        logger.info(f"åˆ†ææ‘˜è¦: æ­£é¢{positive_count}, ä¸­æ€§{neutral_count}, è´Ÿé¢{negative_count}")
        return summary


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    analyzer = FeedbackAnalyzer()
    
    test_feedbacks = [
        "è¿™ä¸ªè½¯ä»¶å¤ªå¡é¡¿äº†ï¼Œç»å¸¸é—ªé€€ï¼Œä½“éªŒå¾ˆå·®",
        "ç•Œé¢è®¾è®¡å¾ˆç¾è§‚ï¼Œç”¨èµ·æ¥å¾ˆèˆ’æœï¼ŒåŠŸèƒ½ä¹Ÿå¾ˆå¼ºå¤§",
        "åŠŸèƒ½ç¼ºå¤±ï¼Œå¸Œæœ›å¢åŠ å¯¼å‡ºåŠŸèƒ½ï¼Œæ“ä½œæœ‰ç‚¹å¤æ‚",
        "éå¸¸å¥½ç”¨ï¼Œæ¨èç»™æœ‹å‹äº†ï¼Œäº”æ˜Ÿå¥½è¯„",
        "ä»·æ ¼å¤ªè´µï¼Œæ€§ä»·æ¯”ä¸é«˜ï¼Œè€Œä¸”è¿˜ç»å¸¸å´©æºƒ"
    ]
    
    # æµ‹è¯•æƒ…æ„Ÿåˆ†æ
    results = analyzer.batch_analyze_sentiment(test_feedbacks)
    for r in results:
        print(f"{r['emotion']} {r['sentiment']} ({r['sentiment_score']:.2f}): {r['text'][:30]}...")
    
    # æµ‹è¯•ç—›ç‚¹æå–
    pain_points = analyzer.extract_pain_points(test_feedbacks, topK=10)
    print(f"\né«˜é¢‘ç—›ç‚¹è¯æ±‡: {pain_points}")
    
    # æµ‹è¯•æ‘˜è¦
    summary = analyzer.generate_summary(results)
    print(f"\nåˆ†ææ‘˜è¦: {summary}")
